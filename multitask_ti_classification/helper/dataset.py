# dataset.py

import torch
from torch.utils.data import Dataset
from torch_geometric.data import Data as PyGData
from torch_geometric.loader import DataLoader as PyGDataLoader # For handling PyGData objects in batches
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
from sklearn.preprocessing import StandardScaler
from collections import defaultdict
import warnings

# Import from local modules
import helper.config as config
from utils import SpaceGroupManager, load_material_graph_from_dict, load_pickle_data

# Suppress specific warnings from pandas when mapping values that might not exist
warnings.filterwarnings("ignore", ".*is not in the top-level domain.*", UserWarning)


class MaterialDataset(Dataset):
    def __init__(self, master_index_path: Union[str, Path], kspace_graphs_base_dir: Union[str, Path],
                 data_root_dir: Union[str, Path], scaler: Optional[StandardScaler] = None):
        """
        Args:
            master_index_path: Path to the master_index.parquet file generated by MultiModalMaterialDatabase.
            kspace_graphs_base_dir: Base directory where k-space graphs are stored by space group.
            data_root_dir: The root directory for the entire multimodal database (e.g., 'multimodal_materials_db_mp').
                           Used to resolve relative paths in master_index.
            scaler: A pre-fitted StandardScaler for numerical features. If None, the dataset can be initialized
                    without a scaler, and one can be fitted and assigned later.
        """
        self.master_index_path = Path(master_index_path)
        self.kspace_graphs_base_dir = Path(kspace_graphs_base_dir)
        self.data_root_dir = Path(data_root_dir)

        if not self.master_index_path.exists():
            raise FileNotFoundError(f"Master index file not found at: {self.master_index_path}")
        
        self.metadata_df = pd.read_parquet(self.master_index_path)
        
        # Filter out materials that don't have valid labels in our defined mappings
        initial_count = len(self.metadata_df)
        self.metadata_df = self.metadata_df[
            self.metadata_df['topological_class'].isin(config.TOPOLOGY_CLASS_MAPPING.keys()) &
            self.metadata_df['magnetic_type'].isin(config.MAGNETISM_CLASS_MAPPING.keys())
        ].reset_index(drop=True)
        if len(self.metadata_df) < initial_count:
            print(f"Filtered out {initial_count - len(self.metadata_df)} materials due to undefined topological or magnetic types.")

        self.space_group_manager = SpaceGroupManager(self.kspace_graphs_base_dir)
        self.scaler = scaler

        self.topology_class_map = config.TOPOLOGY_CLASS_MAPPING
        self.magnetism_class_map = config.MAGNETISM_CLASS_MAPPING

        # Define which scalar features to extract from the metadata.df
        # These columns should directly exist in your master_index.parquet or be accessible via `row['nested.key']`
        self.scalar_features_columns = [
            'band_gap', 'formation_energy', 'density', 'volume', 'nsites',
            'space_group_number', 'total_magnetization', 'energy_above_hull'
        ]
        
        # Initialize feature dimensions (will be updated dynamically by train.py after first item load)
        self._crystal_node_feature_dim = None
        self._kspace_graph_node_feature_dim = None
        self._asph_feature_dim = None
        self._scalar_total_dim = None

    def __len__(self) -> int:
        return len(self.metadata_df)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Loads and processes data for a single material at the given index.
        """
        row = self.metadata_df.iloc[idx]
        
        # --- 1. Load Crystal Graph ---
        crystal_graph_path = self.data_root_dir / row['file_locations.crystal_graph']
        crystal_graph_dict = load_pickle_data(crystal_graph_path)
        crystal_graph = load_material_graph_from_dict(crystal_graph_dict)
        
        # --- 2. Load ASPH Features ---
        asph_features_path = self.data_root_dir / row['file_locations.point_cloud']
        asph_features = torch.tensor(np.load(asph_features_path), dtype=torch.float)

        # --- 3. Load K-space Graph and Physics Features (from shared files) ---
        sg_number = row['space_group_number']
        kspace_data_tuple = self.space_group_manager.load_kspace_data(sg_number)

        kspace_graph = None
        kspace_physics_features = None

        if kspace_data_tuple:
            kspace_graph, kspace_physics_features = kspace_data_tuple
        else:
            # Fallback for missing k-space data. Use dummy tensors to maintain batch consistency.
            # Dimensions should match what the model expects even for dummy data.
            print(f"Warning: Missing shared k-space graph/features for SG {sg_number} (JID: {row['jid']}). Using dummy data.")
            # Ensure dummy dimensions align with expected model input if possible.
            # These specific dimensions might need to be adjusted based on actual model input expectations.
            dummy_node_dim = self._kspace_graph_node_feature_dim if self._kspace_graph_node_feature_dim else config.KSPACE_GRAPH_NODE_FEATURE_DIM # Fallback to config if not yet inferred
            dummy_global_dim = config.BAND_REP_FEATURE_DIM + 3 + 1 # (band_rep_vector + electric_field + space_group_number)
            
            kspace_graph = PyGData(x=torch.zeros(1, dummy_node_dim),
                                   edge_index=torch.empty(2, 0, dtype=torch.long),
                                   u=torch.zeros(1, dummy_global_dim))
            kspace_physics_features = { # Matches KSpacePhysicsGraphBuilder._create_physics_features_tensor structure
                'ebr_features': torch.zeros(5, dtype=torch.float),
                'topological_indices': torch.zeros(5, dtype=torch.float),
                'decomposition_features': torch.zeros(2, dtype=torch.float)
            }


        # --- 4. Extract Scalar Features (Band Reps + Metadata) ---
        # Note: 'vectorized_features_dir' is a directory, not a direct file.
        # Your previous code extracts 'band_rep_features.npy' from there.
        band_rep_features_path = self.data_root_dir / row['file_locations.vectorized_features_dir'] / 'band_rep_features.npy'
        band_rep_features = torch.tensor(np.load(band_rep_features_path), dtype=torch.float)
        
        # Extract direct scalar metadata features from the row
        scalar_metadata_features = [row[col] for col in self.scalar_features_columns]
        # Handle potential NaN values: replace with 0 or a sensible default
        scalar_metadata_features = [0.0 if pd.isna(val) else val for val in scalar_metadata_features]
        scalar_metadata_features = torch.tensor(scalar_metadata_features, dtype=torch.float)

        # Concatenate band_rep_features with other scalar metadata features
        combined_scalar_features = torch.cat([band_rep_features, scalar_metadata_features])
        
        # Apply scaling if scaler is provided
        if self.scaler:
            # StandardScaler expects a 2D array (n_samples, n_features)
            combined_scalar_features = torch.tensor(self.scaler.transform(combined_scalar_features.unsqueeze(0)).squeeze(0), dtype=torch.float)
        
        # Dynamically set feature dimensions after the first item is loaded
        if self._crystal_node_feature_dim is None:
            self._crystal_node_feature_dim = crystal_graph.x.shape[1]
            config.CRYSTAL_NODE_FEATURE_DIM = self._crystal_node_feature_dim
        if self._kspace_graph_node_feature_dim is None:
            self._kspace_graph_node_feature_dim = kspace_graph.x.shape[1]
            config.KSPACE_GRAPH_NODE_FEATURE_DIM = self._kspace_graph_node_feature_dim
        if self._asph_feature_dim is None:
            self._asph_feature_dim = asph_features.shape[0]
            config.ASPH_FEATURE_DIM = self._asph_feature_dim
        if self._scalar_total_dim is None:
            self._scalar_total_dim = combined_scalar_features.shape[0]
            config.SCALAR_TOTAL_DIM = self._scalar_total_dim

        # --- 5. Prepare Labels ---
        topology_label_str = row['topological_class']
        topology_label = torch.tensor(self.topology_class_map.get(topology_label_str, self.topology_class_map["Unknown"]), dtype=torch.long)
        
        magnetism_label_str = row['magnetic_type']
        magnetism_label = torch.tensor(self.magnetism_class_map.get(magnetism_label_str, self.magnetism_class_map["UNKNOWN"]), dtype=torch.long)

        return {
            'crystal_graph': crystal_graph,
            'kspace_graph': kspace_graph, # This is the PyGData for k-space connectivity
            'asph_features': asph_features,
            'scalar_features': combined_scalar_features, # This includes band_rep and metadata
            'kspace_physics_features': kspace_physics_features, # This is the dict of tensors (ebr, topo_indices, decomp_features)
            'topology_label': topology_label,
            'magnetism_label': magnetism_label,
            'jid': row['jid'] # Include JID for debugging/tracking
        }

# Custom collate function for PyGDataLoader to handle dictionary of Data objects and other tensors
def custom_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Custom collate function for PyGDataLoader to handle a dictionary of inputs.
    It will batch PyGData objects separately and stack other tensors.
    """
    if not batch:
        return {}

    # Separate PyGData objects for batching with PyG's default_collate
    # PyGDataLoader.dataset is a hacky way to access the default_collate function.
    # More robust way would be to import default_collate directly from torch_geometric.data.dataloader
    # For now, this works as PyGDataLoader internally uses it for its dataset if given a list of Data objects.
    
    # Batch crystal graphs
    crystal_graphs_batch = PyGDataLoader(
        [d['crystal_graph'] for d in batch],
        batch_size=len(batch) # Process all items in this batch
    ).dataset 
    # Batch kspace graphs
    kspace_graphs_batch = PyGDataLoader(
        [d['kspace_graph'] for d in batch],
        batch_size=len(batch)
    ).dataset

    collated_batch = {
        'crystal_graph': crystal_graphs_batch,
        'kspace_graph': kspace_graphs_batch,
        'asph_features': torch.stack([d['asph_features'] for d in batch]),
        'scalar_features': torch.stack([d['scalar_features'] for d in batch]),
        'topology_label': torch.stack([d['topology_label'] for d in batch]),
        'magnetism_label': torch.stack([d['magnetism_label'] for d in batch]),
        'jid': [d['jid'] for d in batch]
    }

    # Handle kspace_physics_features which is a dict of tensors
    # Collect all sub-features into lists and then stack them
    kspace_physics_features_collated = defaultdict(list)
    for d in batch:
        for key, tensor in d['kspace_physics_features'].items():
            kspace_physics_features_collated[key].append(tensor)
    
    # Now stack the collected lists of tensors
    for key in kspace_physics_features_collated:
        kspace_physics_features_collated[key] = torch.stack(kspace_physics_features_collated[key])
    collated_batch['kspace_physics_features'] = kspace_physics_features_collated

    return collated_batch